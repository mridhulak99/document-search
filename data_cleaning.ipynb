{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63517587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mridhula\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mridhula\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from functools import reduce\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89e4b8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseInputFilePath = \"project/input-files\"\n",
    "baseInputTransform = \"project/input-transform\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19e0f0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemm tockenize the words\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def stemming(text):\n",
    "    return reduce(lambda x, y: x + \" \" + ps.stem(y), text, \"\")\n",
    "\n",
    "def stopWordsRemoval(text):\n",
    "    words = text.split() \n",
    "    filtered_words = [word for word in words if word not in stop_words] \n",
    "    return filtered_words\n",
    "\n",
    "def removeNonAlpha(text):\n",
    "    return \" \".join([word for word in text if word.isalpha()])\n",
    "    \n",
    "def process(text):\n",
    "    text = tokenize(text)\n",
    "    text = stemming(text)\n",
    "    text = stopWordsRemoval(text)\n",
    "    text = removeNonAlpha(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51f9e232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'document describ market strategi carri compani agricultur chemic report predict market share chemic report market statist agrochem pesticid herbicid fungicid insecticid fertil predict sale market share stimul demand price cut volum sale'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''Document will describe marketing strategies carried out by U.S. companies for their agricultural \n",
    "chemicals, report predictions for market share of such chemicals, or report market statistics for \n",
    "agrochemicals, pesticide, herbicide, fungicide, insecticide, fertilizer, predicted sales, market share, \n",
    "stimulate demand, price cut, volume of sales. '''\n",
    "process(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba1d7bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the path create a folder with only the stemmed words\n",
    "def filePathRecurse(basePath, basePathParent, createPath):\n",
    "    if basePath[-4:] == \".zip\":\n",
    "        print(basePath)\n",
    "        ZipFile(basePath).extractall(basePathParent)\n",
    "        di = os.listdir(basePathParent)[0]\n",
    "        if di[-4:]=='.txt' or di[-4:]=='.TXT':\n",
    "            newfilepath = basePathParent + '/' +  os.listdir(basePathParent)[0]\n",
    "        else:\n",
    "            newfilepath = basePathParent + '/' +  os.listdir(basePathParent)[0]\n",
    "            newfilepath = newfilepath + '/' + os.listdir(newfilepath)[0]\n",
    "        text = Path(newfilepath).read_text().replace('\\n', ' ')\n",
    "        transform = process(text)\n",
    "        transformfilepath = createPath[:-4] + \".txt\"\n",
    "        saveFile = open(transformfilepath, 'w')\n",
    "        saveFile.write(transform)\n",
    "        saveFile.close()\n",
    "        return\n",
    "        \n",
    "    if basePath[-4:] == \".txt\":\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(createPath):\n",
    "        os.makedirs(createPath)\n",
    "    try:\n",
    "        dirList = os.listdir(basePath)\n",
    "    except Exception as e:\n",
    "        t\n",
    "    for path in dirList:\n",
    "        filePathRecurse(basePath + '/' + path , basePath, createPath + '/' + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef1deace",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project/input-files/aleph.gutenberg.org/1/0/0/0/10001/10001.zip\n",
      "project/input-files/aleph.gutenberg.org/1/0/0/0/10002/10002.zip\n",
      "project/input-files/aleph.gutenberg.org/1/0/0/0/10003/10003.zip\n",
      "project/input-files/aleph.gutenberg.org/1/0/0/0/10004/10004.zip\n",
      "project/input-files/aleph.gutenberg.org/1/0/0/0/10005/10005.zip\n",
      "project/input-files/aleph.gutenberg.org/1/0/0/0/10006/10006.zip\n",
      "project/input-files/aleph.gutenberg.org/1/0/0/0/10007/10007.zip\n",
      "project/input-files/aleph.gutenberg.org/1/0/0/0/10008/10008.zip\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m filePathRecurse(baseInputFilePath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, baseInputTransform)\n",
      "Cell \u001b[1;32mIn[5], line 30\u001b[0m, in \u001b[0;36mfilePathRecurse\u001b[1;34m(basePath, basePathParent, createPath)\u001b[0m\n\u001b[0;32m     28\u001b[0m     t\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m dirList:\n\u001b[1;32m---> 30\u001b[0m     filePathRecurse(basePath \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m path , basePath, createPath \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m path)\n",
      "Cell \u001b[1;32mIn[5], line 30\u001b[0m, in \u001b[0;36mfilePathRecurse\u001b[1;34m(basePath, basePathParent, createPath)\u001b[0m\n\u001b[0;32m     28\u001b[0m     t\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m dirList:\n\u001b[1;32m---> 30\u001b[0m     filePathRecurse(basePath \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m path , basePath, createPath \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m path)\n",
      "    \u001b[1;31m[... skipping similar frames: filePathRecurse at line 30 (4 times)]\u001b[0m\n",
      "Cell \u001b[1;32mIn[5], line 30\u001b[0m, in \u001b[0;36mfilePathRecurse\u001b[1;34m(basePath, basePathParent, createPath)\u001b[0m\n\u001b[0;32m     28\u001b[0m     t\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m dirList:\n\u001b[1;32m---> 30\u001b[0m     filePathRecurse(basePath \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m path , basePath, createPath \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m path)\n",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m, in \u001b[0;36mfilePathRecurse\u001b[1;34m(basePath, basePathParent, createPath)\u001b[0m\n\u001b[0;32m     11\u001b[0m     newfilepath \u001b[38;5;241m=\u001b[39m newfilepath \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(newfilepath)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     12\u001b[0m text \u001b[38;5;241m=\u001b[39m Path(newfilepath)\u001b[38;5;241m.\u001b[39mread_text()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m transform \u001b[38;5;241m=\u001b[39m process(text)\n\u001b[0;32m     14\u001b[0m transformfilepath \u001b[38;5;241m=\u001b[39m createPath[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m saveFile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(transformfilepath, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m, in \u001b[0;36mprocess\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(text):\n\u001b[1;32m---> 17\u001b[0m     text \u001b[38;5;241m=\u001b[39m tokenize(text)\n\u001b[0;32m     18\u001b[0m     text \u001b[38;5;241m=\u001b[39m stemming(text)\n\u001b[0;32m     19\u001b[0m     text \u001b[38;5;241m=\u001b[39m stopWordsRemoval(text)\n",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m, in \u001b[0;36mtokenize\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(text):\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m word_tokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1281\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1278\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentences_from_text(text, realign_boundaries))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1341\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[0;32m   1333\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1341\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[0;32m   1333\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1329\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n\u001b[0;32m   1328\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[1;32m-> 1329\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m slices:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (sentence\u001b[38;5;241m.\u001b[39mstart, sentence\u001b[38;5;241m.\u001b[39mstop)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1459\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[0;32m   1448\u001b[0m \u001b[38;5;124;03mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1458\u001b[0m realign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1459\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence1, sentence2 \u001b[38;5;129;01min\u001b[39;00m _pair_iter(slices):\n\u001b[0;32m   1460\u001b[0m     sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(sentence1\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m realign, sentence1\u001b[38;5;241m.\u001b[39mstop)\n\u001b[0;32m   1461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentence2:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:324\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (prev, el)\n\u001b[0;32m    326\u001b[0m     prev \u001b[38;5;241m=\u001b[39m el\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1432\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1430\u001b[0m last_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1431\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_potential_end_contexts(text):\n\u001b[1;32m-> 1432\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[0;32m   1433\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(last_break, match\u001b[38;5;241m.\u001b[39mend())\n\u001b[0;32m   1434\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_tok\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1435\u001b[0m             \u001b[38;5;66;03m# next sentence starts after whitespace\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1480\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.text_contains_sentbreak\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1476\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1477\u001b[0m \u001b[38;5;124;03mReturns True if the given text includes a sentence break.\u001b[39;00m\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1479\u001b[0m found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# used to ignore last token\u001b[39;00m\n\u001b[1;32m-> 1480\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_annotate_tokens(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenize_words(text)):\n\u001b[0;32m   1481\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m found:\n\u001b[0;32m   1482\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1622\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._annotate_second_pass\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m   1614\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_annotate_second_pass\u001b[39m(\n\u001b[0;32m   1615\u001b[0m     \u001b[38;5;28mself\u001b[39m, tokens: Iterator[PunktToken]\n\u001b[0;32m   1616\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[PunktToken]:\n\u001b[0;32m   1617\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1618\u001b[0m \u001b[38;5;124;03m    Performs a token-based classification (section 4) over the given\u001b[39;00m\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;124;03m    tokens, making use of the orthographic heuristic (4.1.1), collocation\u001b[39;00m\n\u001b[0;32m   1620\u001b[0m \u001b[38;5;124;03m    heuristic (4.1.2) and frequent sentence starter heuristic (4.1.3).\u001b[39;00m\n\u001b[0;32m   1621\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1622\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token1, token2 \u001b[38;5;129;01min\u001b[39;00m _pair_iter(tokens):\n\u001b[0;32m   1623\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_second_pass_annotation(token1, token2)\n\u001b[0;32m   1624\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m token1\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:313\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;124;03m\"\"\"Matches token types that are not merely punctuation. (Types for\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;124;03mnumeric tokens are changed to ##number## and hence contain alpha.)\"\"\"\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;66;03m# }\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;66;03m# { Helper Functions\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;66;03m# ////////////////////////////////////////////////////////////\u001b[39;00m\n\u001b[1;32m--> 313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pair_iter\u001b[39m(iterator):\n\u001b[0;32m    314\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m    Yields pairs of tokens from the given iterator such that each input\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03m    token will appear as the first element in a yielded tuple. The last\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;124;03m    pair will have None as its second element.\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterator)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filePathRecurse(baseInputFilePath, \"\", baseInputTransform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db60afe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868fc9f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
